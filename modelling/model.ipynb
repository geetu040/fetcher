{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset_url, map_url, _pos=False, _prev=False, _can=False, _sparse=False, cnn_layers=None):\n",
    "    df = pd.read_csv(dataset_url)\n",
    "    with open(map_url, 'r') as f:\n",
    "        map = json.load(f)\n",
    "\n",
    "    X_all = []\n",
    "    if _pos:\n",
    "        X = df[['fet_x', 'fet_y', 'tar_x', 'tar_y']]\n",
    "        X.fet_x /= 27\n",
    "        X.fet_y /= 31\n",
    "        X.tar_x /= 27\n",
    "        X.tar_y /= 31\n",
    "        X_all.append(X)\n",
    "    if _prev:\n",
    "        X = pd.get_dummies(df.prev_dir).iloc[:, :-1]\n",
    "        X_all.append(X)\n",
    "    if _can:\n",
    "        X = []\n",
    "        for i in np.array([df.fet_x, df.fet_y, df.prev_dir]).T:\n",
    "            possible_dirs = [\n",
    "                int([i[0], i[1]-1] in map),\n",
    "                int([i[0]-1, i[1]] in map),\n",
    "                int([i[0], i[1]+1] in map),\n",
    "                int([i[0]+1, i[1]] in map),\n",
    "            ]\n",
    "            if i[2] != 4:\n",
    "                possible_dirs[(i[2]+2) % 4] = 0\n",
    "            X.append(possible_dirs)\n",
    "        X = pd.DataFrame(X, columns=['can_u', 'can_l', 'can_d', 'can_r'])\n",
    "        X_all.append(X)\n",
    "\n",
    "    if _sparse:\n",
    "        sparse_matrix = []\n",
    "        for fet_x, fet_y, tar_x, tar_y in zip(df.fet_x, df.fet_y, df.tar_x, df.tar_y):\n",
    "            map_grid = []\n",
    "            for i in range(1, 29):\n",
    "                row = []\n",
    "                for j in range(1, 32):\n",
    "                    if [i, j] not in map:\n",
    "                        row.append([1, 0, 0, 0])\n",
    "                    elif [i, j] == [fet_x, fet_y]:\n",
    "                        row.append([0, 0, 1, 0])\n",
    "                    elif [i, j] == [tar_x, tar_y]:\n",
    "                        row.append([0, 0, 0, 1])\n",
    "                    else:\n",
    "                        row.append([0, 1, 0, 0])\n",
    "                map_grid.append(row)\n",
    "\n",
    "            sparse_matrix.append(map_grid)\n",
    "        sparse_matrix = np.array(sparse_matrix)\n",
    "        global sm\n",
    "        sm = sparse_matrix\n",
    "        model_cnn = models.Sequential([\n",
    "            layers.InputLayer(input_shape=(*sparse_matrix.shape[1:],)),\n",
    "            *cnn_layers,\n",
    "            layers.Flatten()\n",
    "        ])\n",
    "        X = model_cnn(sparse_matrix)\n",
    "        X_all.append(X)\n",
    "\n",
    "    \n",
    "    y = df.pop('dir')\n",
    "    return X_all, y\n",
    "\n",
    "def plot_it(history):\n",
    "    history = pd.DataFrame(history.history)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    axes[0].plot(history['acc'], color='black')\n",
    "    axes[0].plot(history['val_acc'], color='red')\n",
    "    axes[0].legend(['acc', 'val_acc'])\n",
    "    axes[0].set_ylabel(\"acc\")\n",
    "    axes[0].set_xlabel(\"epochs\")\n",
    "    axes[1].plot(history['loss'][len(history['loss'])//4:], color='black')\n",
    "    axes[1].plot(history['val_loss'][len(history['loss'])//4:], color='red')\n",
    "    axes[1].legend(['loss', 'val_loss'])\n",
    "    axes[1].set_xlabel(\"epochs\")\n",
    "    axes[1].set_ylabel(\"loss\")\n",
    "    plt.show()\n",
    "\n",
    "def print_matrics(history):\n",
    "    vl = np.argmin(history.history['val_loss'])\n",
    "    l = np.argmin(history.history['loss'])\n",
    "    a = np.argmax(history.history['acc'])\n",
    "    va = np.argmax(history.history['val_acc'])\n",
    "\n",
    "    print(f'''\n",
    "        --> accuray\n",
    "        index:\\t\\t{a}\n",
    "        acc:\\t\\t{history.history['acc'][a]}\n",
    "        val_acc:\\t{history.history['val_acc'][a]}\n",
    "        loss:\\t\\t{history.history['loss'][a]}\n",
    "        val_loss:\\t{history.history['val_loss'][a]}\n",
    "        --> validatoin accuracy\n",
    "        index:\\t\\t{va}\n",
    "        acc:\\t\\t{history.history['acc'][va]}\n",
    "        val_acc:\\t{history.history['val_acc'][va]}\n",
    "        loss:\\t\\t{history.history['loss'][va]}\n",
    "        val_loss:\\t{history.history['val_loss'][va]}\n",
    "        --> validation loss\n",
    "        index:\\t\\t{vl}\n",
    "        acc:\\t\\t{history.history['acc'][vl]}\n",
    "        val_acc:\\t{history.history['val_acc'][vl]}\n",
    "        loss:\\t\\t{history.history['loss'][vl]}\n",
    "        val_loss:\\t{history.history['val_loss'][vl]}\n",
    "        --> loss\n",
    "        index:\\t\\t{l}\n",
    "        acc:\\t\\t{history.history['acc'][l]}\n",
    "        val_acc:\\t{history.history['val_acc'][l]}\n",
    "        loss:\\t\\t{history.history['loss'][l]}\n",
    "        val_loss:\\t{history.history['val_loss'][l]}\n",
    "    ''')\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    with open(model_name + '_model.json', 'r') as f:\n",
    "        content = f.read()\n",
    "        loaded_model_acc = models.model_from_json(content)\n",
    "        loaded_model_val_acc = models.model_from_json(content)\n",
    "        loaded_model_val_loss = models.model_from_json(content)\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    print('acc')\n",
    "    loaded_model_acc.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "    loaded_model_acc.load_weights(model_name + '_weights_acc.h5')\n",
    "    loss, acc = loaded_model_acc.evaluate(X, y)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    print('val_acc')\n",
    "    loaded_model_val_acc.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "    loaded_model_val_acc.load_weights(model_name + '_weights_val_acc.h5')\n",
    "    loss, acc = loaded_model_val_acc.evaluate(X, y)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    print('val_loss')\n",
    "    loaded_model_val_acc.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "    loaded_model_val_acc.load_weights(model_name + '_weights_val_loss.h5')\n",
    "    loss, acc = loaded_model_val_acc.evaluate(X, y)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    print(\"\\nbest: \" + ['acc', 'val_acc', 'val_loss'][accuracies.index(max(accuracies))])\n",
    "    print(max(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers, callbacks, optimizers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json, joblib\n",
    "from google.colab import files\n",
    "\n",
    "model_name = 'manual'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y = preprocess(\n",
    "\tmodel_name + '_dataset.csv',\n",
    "\t'mapping.json',\n",
    "\t_pos=True, _prev=False, _can=True, _sparse=False,\n",
    "\tcnn_layers = [\n",
    "\tlayers.Conv2D(32, 2, padding='same', activation='relu'),\n",
    "\tlayers.Conv2D(64, 2, padding='same', activation='relu'),\n",
    "\t]\n",
    ")\n",
    "X = np.column_stack(X_all)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(units=400, activation='relu', input_shape=(X.shape[1],)),\n",
    "    layers.Dense(units=700, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=80, activation='relu'),\n",
    "    layers.Dense(units=4, activation='sigmoid'),\n",
    "])\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "early_callback = callbacks.EarlyStopping(patience=300, monitor='acc', mode='max')\n",
    "save_callback_acc = callbacks.ModelCheckpoint(\n",
    "\tfilepath= model_name + '_weights_acc.h5',\n",
    "\tsave_weights_only=True,\n",
    "\tfrequency='epoch',\n",
    "\tsave_best_only=True,\n",
    "\tmonitor='acc',\n",
    "\tmode='max',\n",
    ")\n",
    "save_callback_val_acc = callbacks.ModelCheckpoint(\n",
    "\tfilepath= model_name + '_weights_val_acc.h5',\n",
    "\tsave_weights_only=True,\n",
    "\tfrequency='epoch',\n",
    "\tsave_best_only=True,\n",
    "\tmonitor='val_acc',\n",
    "\tmode='max',\n",
    ")\n",
    "save_callback_val_loss = callbacks.ModelCheckpoint(\n",
    "\tfilepath= model_name + '_weights_val_loss.h5',\n",
    "\tsave_weights_only=True,\n",
    "\tfrequency='epoch',\n",
    "\tsave_best_only=True,\n",
    "\tmonitor='val_loss',\n",
    "\tmode='min',\n",
    ")\n",
    "with open(model_name + '_model.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "history = model.fit(X, y, epochs=2000, validation_split=0.25, verbose=2, callbacks=[early_callback, save_callback_acc, save_callback_val_acc, save_callback_val_loss])\n",
    "plot_it(history)\n",
    "print_matrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency of Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "def warn(*args, **kwargs): pass\n",
    "warnings.warn = warn\n",
    "\n",
    "sklearn_models = {\n",
    "    'knn': KNeighborsClassifier(n_neighbors=25, weights='uniform', metric=\"minkowski\", p=2),\n",
    "    'scv': SVC(C = 10, kernel = \"poly\"),\n",
    "    'log': LogisticRegression(),\n",
    "    'logcv': LogisticRegressionCV(),\n",
    "    'tree': DecisionTreeClassifier(criterion=\"entropy\"),\n",
    "    'forest': RandomForestClassifier(n_estimators=10, criterion=\"entropy\"),\n",
    "    'gaus': GaussianNB(),\n",
    "}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "sklearn_predictions = {}\n",
    "sklearn_accuracies = {}\n",
    "for key in sklearn_models:\n",
    "    sklearn_models[key].fit(X_train, y_train)\n",
    "    pred = sklearn_models[key].predict(X_train)\n",
    "    sklearn_predictions[key] = pred\n",
    "    sklearn_accuracies[key] = accuracy_score(y_train, pred)\n",
    "\n",
    "print(sklearn_accuracies)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
